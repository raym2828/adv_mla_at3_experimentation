{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 02:51:04.369525: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-06 02:51:04.840371: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 02:51:06.042236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "import joblib\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open master df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open master\n",
    "df_train= pd.read_feather('../data/processed/train_data.feather')\n",
    "df_test= pd.read_feather('../data/processed/test_data.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average price per route\n",
    "route_avg_price = (\n",
    "    df_train.groupby(['startingAirport', 'destinationAirport'])['totalFare']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'totalFare': 'average_price'})\n",
    ")\n",
    "\n",
    "\n",
    "# Merge this back to the original dataset\n",
    "df_train = df_train.merge(route_avg_price, on=['startingAirport', 'destinationAirport'], how='left')\n",
    "df_test = df_test.merge(route_avg_price, on=['startingAirport', 'destinationAirport'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average distance to the dataset\n",
    "route_avg_distance = (\n",
    "    df_train.groupby(['startingAirport', 'destinationAirport'])['totalTravelDistance']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'totalTravelDistance': 'average_distance'})\n",
    ")\n",
    "\n",
    "df_train = df_train.merge(route_avg_distance, on=['startingAirport', 'destinationAirport'], how='left')\n",
    "df_test = df_test.merge(route_avg_distance, on=['startingAirport', 'destinationAirport'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique routes in the dataset\n",
    "# Create a new column to represent the unique route, combining airports alphabetically, store as string\n",
    "def unique_route(df):\n",
    "    df['route'] = df[['startingAirport', 'destinationAirport']].apply(\n",
    "        lambda x: str(tuple(sorted(x))), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_train = unique_route(df_train)\n",
    "df_test = unique_route(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. Encode the categories\n",
    "2. Normalise\n",
    "3. Split and train\n",
    "4. train \n",
    "5. Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess the features\n",
    "# Define the features and target\n",
    "def preprocess_drop(df):\n",
    "    df = df.drop([ 'searchDate', 'flightDate','segmentsArrivalAirportCode'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Label encode for airports\n",
    "def process_airports(df):\n",
    "    le = LabelEncoder()\n",
    "    all_airports = sorted(set(df['startingAirport']).union(df['destinationAirport']))\n",
    "    le.fit(all_airports)\n",
    "    \n",
    "    df['startingAirport'] = le.transform(df['startingAirport'])\n",
    "    df['destinationAirport'] = le.transform(df['destinationAirport'])\n",
    "    # Print dictionary of the label encoder for airports with original values and the encoded values\n",
    "    print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "    return df, le\n",
    "\n",
    "# Label encode for routes\n",
    "def process_routes(df):\n",
    "    le_route = LabelEncoder()\n",
    "    df['route'] = le_route.fit_transform(df['route'])\n",
    "    return df, le_route\n",
    "    \n",
    "\n",
    "\n",
    "#Features to process\n",
    "boolean_cols = ['isNonStop']\n",
    "ohe_cols = ['AirlineNameScore', 'CabinCode']\n",
    "scale_cols = ['DepartureTimeHour','date_diff_days', 'CabinCode','average_distance', 'average_price']\n",
    "scale_cols = list(set(scale_cols) - set(ohe_cols))\n",
    "\n",
    "# Process boolean columns\n",
    "def process_boolean(df, boolean_cols):\n",
    "    df[boolean_cols] = df[boolean_cols].astype(int)\n",
    "    return df\n",
    "\n",
    "# Scale data\n",
    "def process_scale(df, scale_cols):\n",
    "    scaler = StandardScaler()\n",
    "    print('scale_cols',scale_cols)\n",
    "    print('df[scale_cols]',df.columns)\n",
    "    df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
    "    return df, scaler\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "def process_ohe(df, ohe_cols, ohe=None):\n",
    "    if ohe is None:\n",
    "        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        ohe.fit(df[ohe_cols])\n",
    "    ohe_df = pd.DataFrame(ohe.transform(df[ohe_cols]), columns=ohe.get_feature_names_out(ohe_cols))\n",
    "    df = df.drop(ohe_cols, axis=1)\n",
    "    df = pd.concat([df, ohe_df], axis=1)\n",
    "    return df, ohe\n",
    "\n",
    "# Process test data\n",
    "def process_test_data(df, le, le_route, scaler,  boolean_cols, scale_cols, ohe_cols):\n",
    "    print('le',le)\n",
    "    df['startingAirport'] = le.transform(df['startingAirport'])\n",
    "    df['destinationAirport'] = le.transform(df['destinationAirport'])\n",
    "    print('le_route',le_route)\n",
    "    df['route'] = le_route.transform(df['route'])\n",
    "    df = process_boolean(df, boolean_cols)\n",
    "    print('processboolean',df.columns)\n",
    "    df[scale_cols] = scaler.transform(df[scale_cols])\n",
    "    # df, _ = process_ohe(df, ohe_cols, ohe)\n",
    "    return df\n",
    "\n",
    "# # weekday to get cos and sine\n",
    "# def process_weekday(df):\n",
    "#     df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "#     df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "#     df.drop('weekday', axis=1, inplace=True)\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the features\n",
    "def preprocess_features(df):\n",
    "    # df, le = process_airports(df)\n",
    "    # print the columns in df \n",
    "    print('processairport',df.columns)\n",
    "    df = process_boolean(df)\n",
    "    print('processboolean',df.columns)\n",
    "    df = process_weekday(df)\n",
    "    print('processweekday',df.columns)\n",
    "    df = process_ohe(df)\n",
    "    print('processohe',df.columns)\n",
    "    df, scaler = process_scale(df)\n",
    "    print('processscale',df.columns)\n",
    "    \n",
    "    return df, le, scaler\n",
    "\n",
    "def preprocess_features_test(df):\n",
    "    # df = process_airports_test(df)\n",
    "    df = process_boolean(df)\n",
    "    df = process_weekday(df)\n",
    "    df = process_ohe(df)\n",
    "    df = process_scale_test(df)\n",
    "    return df\n",
    "\n",
    "def prepar_data_set(data_df):\n",
    "    categoy_features = ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route']\n",
    "    numerique_features = ['DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']\n",
    "    encoders = {}\n",
    "    for col in categoy_features:\n",
    "        encoder = LabelEncoder()\n",
    "        data_df[col] = encoder.fit_transform(data_df[col])\n",
    "        encoders[col] = encoder\n",
    "    return data_df,categoy_features,numerique_features, encoders\n",
    "\n",
    "def prepare_test_set(data_df,categoy_features):\n",
    "    \n",
    "    for col in categoy_features:\n",
    "        print(col)\n",
    "        data_df[col] = encoders[col].transform(data_df[col])\n",
    "    return data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df_train, test_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess_drop(df_train.copy())\n",
    "val = preprocess_drop(df_val.copy())\n",
    "test = preprocess_drop(df_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ATL': 0, 'BOS': 1, 'CLT': 2, 'DEN': 3, 'DFW': 4, 'DTW': 5, 'EWR': 6, 'IAD': 7, 'JFK': 8, 'LAX': 9, 'LGA': 10, 'MIA': 11, 'OAK': 12, 'ORD': 13, 'PHL': 14, 'SFO': 15}\n",
      "scale_cols ['DepartureTimeHour', 'average_distance', 'average_price', 'date_diff_days']\n",
      "df[scale_cols] Index(['startingAirport', 'destinationAirport', 'isNonStop', 'totalFare',\n",
      "       'totalTravelDistance', 'DepartureTimeHour', 'CabinCode',\n",
      "       'AirlineNameScore', 'date_diff_days', 'weekday', 'average_price',\n",
      "       'average_distance', 'route'],\n",
      "      dtype='object')\n",
      "le LabelEncoder()\n",
      "le_route LabelEncoder()\n",
      "processboolean Index(['startingAirport', 'destinationAirport', 'isNonStop', 'totalFare',\n",
      "       'totalTravelDistance', 'DepartureTimeHour', 'CabinCode',\n",
      "       'AirlineNameScore', 'date_diff_days', 'weekday', 'average_price',\n",
      "       'average_distance', 'route'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Process training data\n",
    "df_train, le_airports = process_airports(train)\n",
    "df_train, le_route = process_routes(df_train)\n",
    "df_train = process_boolean(df_train, boolean_cols)\n",
    "df_train, scaler = process_scale(df_train, scale_cols)\n",
    "# df_train, ohe = process_ohe(df_train, ohe_cols)\n",
    "\n",
    "# Process test data\n",
    "df_test = process_test_data(test, le_airports, le_route, scaler,  boolean_cols, scale_cols, ohe_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorflow nn\n",
    "def create_nn():\n",
    "    # Define the input layers\n",
    "    input_layers = []\n",
    "    output_layers = []\n",
    "    for col in categoy_features:\n",
    "        input_layer = Input(shape=(1,), name=col)\n",
    "        embedding = Embedding(input_dim=int(df_train[col].max()) + 1, output_dim=20)(input_layer)\n",
    "        embedding = Flatten()(embedding)\n",
    "        input_layers.append(input_layer)\n",
    "        output_layers.append(embedding)\n",
    "    for col in numerique_features:\n",
    "        input_layer = Input(shape=(1,), name=col)\n",
    "        input_layers.append(input_layer)\n",
    "        output_layers.append(input_layer)\n",
    "    # Concatenate the layers\n",
    "    x = Concatenate()(output_layers)\n",
    "    x = Dense(192, activation='relu')(x)\n",
    "    x = Dense(288, activation='relu')(x)\n",
    "    x = Dense(192, activation='relu')(x)\n",
    "    x = Dense(1)(x)\n",
    "    model = Model(inputs=input_layers, outputs=x)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raytechie/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route', 'DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']. Received: the structure of inputs={'startingAirport': '*', 'destinationAirport': '*', 'AirlineNameScore': '*', 'CabinCode': '*', 'DepartureTimeHour': '*', 'date_diff_days': '*', 'average_distance': '*', 'average_price': '*', 'weekday': '*', 'route': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6384/6384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 24754.6016 - val_loss: 19774.7695\n",
      "Epoch 2/10\n",
      "\u001b[1m6384/6384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 18459.8047 - val_loss: 18078.0879\n",
      "Epoch 3/10\n",
      "\u001b[1m6384/6384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 17659.1602 - val_loss: 17856.8281\n",
      "Epoch 4/10\n",
      "\u001b[1m6384/6384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 17155.9688 - val_loss: 17243.5703\n",
      "Epoch 5/10\n",
      "\u001b[1m6384/6384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 16729.1914 - val_loss: 17109.9160\n",
      "Epoch 6/10\n",
      "\u001b[1m6384/6384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 16400.7715 - val_loss: 16697.3906\n",
      "Epoch 7/10\n",
      "\u001b[1m6384/6384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 15962.6221 - val_loss: 16176.0361\n",
      "Epoch 8/10\n",
      "\u001b[1m6384/6384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 15903.6963 - val_loss: 16109.5439\n",
      "Epoch 9/10\n",
      "\u001b[1m6384/6384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 15438.2832 - val_loss: 15927.5557\n",
      "Epoch 10/10\n",
      "\u001b[1m6384/6384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 15381.2549 - val_loss: 15740.9268\n",
      "\u001b[1m39896/39896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step\n",
      "Mean Absolute Error: 86.37358093261719\n",
      "Root Mean Squared Error: 123.49488830566406\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=../models/nn/nn_model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoot Mean Squared Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../models/nn/nn_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/saving/saving_api.py:114\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    112\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[1;32m    113\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filepath extension for saving. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=../models/nn/nn_model."
     ]
    }
   ],
   "source": [
    "# Features to use\n",
    "categoy_features = ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route']\n",
    "numerique_features = ['DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']\n",
    "# \n",
    "\n",
    "input_dict={\n",
    "    'startingAirport': df_train['startingAirport'],\n",
    "    'destinationAirport': df_train['destinationAirport'],\n",
    "    'AirlineNameScore': df_train['AirlineNameScore'],\n",
    "    'CabinCode': df_train['CabinCode'],\n",
    "    'DepartureTimeHour': df_train['DepartureTimeHour'],\n",
    "    'date_diff_days': df_train['date_diff_days'],\n",
    "    'average_distance': df_train['average_distance'],\n",
    "    'average_price': df_train['average_price'],\n",
    "    'weekday': df_train['weekday'],\n",
    "    'route': df_train['route']\n",
    "}\n",
    "\n",
    "model = create_nn()\n",
    "history = model.fit(input_dict, df_train['totalFare'], epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "# predict\n",
    "input_dict_test={\n",
    "    'startingAirport': df_test['startingAirport'],\n",
    "    'destinationAirport': df_test['destinationAirport'],\n",
    "    'AirlineNameScore': df_test['AirlineNameScore'],\n",
    "    'CabinCode': df_test['CabinCode'],\n",
    "    'DepartureTimeHour': df_test['DepartureTimeHour'],\n",
    "    'date_diff_days': df_test['date_diff_days'],\n",
    "    'average_distance': df_test['average_distance'],\n",
    "    'average_price': df_test['average_price'],\n",
    "    'weekday': df_test['weekday'],\n",
    "    'route': df_test['route']\n",
    "}\n",
    "\n",
    "y_pred = model.predict(input_dict_test)\n",
    "mae = mean_absolute_error(df_test['totalFare'], y_pred)\n",
    "rmse = root_mean_squared_error(df_test['totalFare'], y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "# Save the model\n",
    "model.save('../models/nn/nn_model.keras')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set predictions and metrics\n",
    "# Process the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
