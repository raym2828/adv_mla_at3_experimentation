{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 04:14:30.309484: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-06 04:14:30.727580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 04:14:32.103610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "import joblib\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open master df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open master\n",
    "df_train= pd.read_feather('../data/processed/train_data.feather')\n",
    "df_test= pd.read_feather('../data/processed/test_data.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average price per route\n",
    "route_avg_price = (\n",
    "    df_train.groupby(['startingAirport', 'destinationAirport'])['totalFare']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'totalFare': 'average_price'})\n",
    ")\n",
    "\n",
    "\n",
    "# Merge this back to the original dataset\n",
    "df_train = df_train.merge(route_avg_price, on=['startingAirport', 'destinationAirport'], how='left')\n",
    "df_test = df_test.merge(route_avg_price, on=['startingAirport', 'destinationAirport'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average distance to the dataset\n",
    "route_avg_distance = (\n",
    "    df_train.groupby(['startingAirport', 'destinationAirport'])['totalTravelDistance']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'totalTravelDistance': 'average_distance'})\n",
    ")\n",
    "\n",
    "df_train = df_train.merge(route_avg_distance, on=['startingAirport', 'destinationAirport'], how='left')\n",
    "df_test = df_test.merge(route_avg_distance, on=['startingAirport', 'destinationAirport'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique routes in the dataset\n",
    "# Create a new column to represent the unique route, combining airports alphabetically, store as string\n",
    "def unique_route(df):\n",
    "    df['route'] = df[['startingAirport', 'destinationAirport']].apply(\n",
    "        lambda x: str(tuple(sorted(x))), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_train = unique_route(df_train)\n",
    "df_test = unique_route(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. Encode the categories\n",
    "2. Normalise\n",
    "3. Split and train\n",
    "4. train \n",
    "5. Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess the features\n",
    "# Define the features and target\n",
    "def preprocess_drop(df):\n",
    "    df = df.drop([ 'searchDate', 'flightDate','segmentsArrivalAirportCode'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Label encode for airports\n",
    "def process_airports(df):\n",
    "    le = LabelEncoder()\n",
    "    all_airports = sorted(set(df['startingAirport']).union(df['destinationAirport']))\n",
    "    le.fit(all_airports)\n",
    "    \n",
    "    df['startingAirport'] = le.transform(df['startingAirport'])\n",
    "    df['destinationAirport'] = le.transform(df['destinationAirport'])\n",
    "    # Print dictionary of the label encoder for airports with original values and the encoded values\n",
    "    print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "    return df, le\n",
    "\n",
    "# Label encode for routes\n",
    "def process_routes(df):\n",
    "    le_route = LabelEncoder()\n",
    "    df['route'] = le_route.fit_transform(df['route'])\n",
    "    return df, le_route\n",
    "    \n",
    "\n",
    "\n",
    "#Features to process\n",
    "boolean_cols = ['isNonStop']\n",
    "ohe_cols = ['AirlineNameScore', 'CabinCode']\n",
    "scale_cols = ['DepartureTimeHour','date_diff_days', 'CabinCode','average_distance', 'average_price']\n",
    "scale_cols = list(set(scale_cols) - set(ohe_cols))\n",
    "\n",
    "# Process boolean columns\n",
    "def process_boolean(df, boolean_cols):\n",
    "    df[boolean_cols] = df[boolean_cols].astype(int)\n",
    "    return df\n",
    "\n",
    "# Scale data\n",
    "def process_scale(df, scale_cols):\n",
    "    scaler = StandardScaler()\n",
    "    print('scale_cols',scale_cols)\n",
    "    print('df[scale_cols]',df.columns)\n",
    "    df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
    "    return df, scaler\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "def process_ohe(df, ohe_cols, ohe=None):\n",
    "    if ohe is None:\n",
    "        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        ohe.fit(df[ohe_cols])\n",
    "    ohe_df = pd.DataFrame(ohe.transform(df[ohe_cols]), columns=ohe.get_feature_names_out(ohe_cols))\n",
    "    df = df.drop(ohe_cols, axis=1)\n",
    "    df = pd.concat([df, ohe_df], axis=1)\n",
    "    return df, ohe\n",
    "\n",
    "# Process test data\n",
    "def process_test_data(df, le, le_route, scaler,  boolean_cols, scale_cols, ohe_cols):\n",
    "    print('le',le)\n",
    "    df['startingAirport'] = le.transform(df['startingAirport'])\n",
    "    df['destinationAirport'] = le.transform(df['destinationAirport'])\n",
    "    print('le_route',le_route)\n",
    "    df['route'] = le_route.transform(df['route'])\n",
    "    df = process_boolean(df, boolean_cols)\n",
    "    print('processboolean',df.columns)\n",
    "    df[scale_cols] = scaler.transform(df[scale_cols])\n",
    "    # df, _ = process_ohe(df, ohe_cols, ohe)\n",
    "    return df\n",
    "\n",
    "# # weekday to get cos and sine\n",
    "# def process_weekday(df):\n",
    "#     df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "#     df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "#     df.drop('weekday', axis=1, inplace=True)\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the features\n",
    "def preprocess_features(df):\n",
    "    # df, le = process_airports(df)\n",
    "    # print the columns in df \n",
    "    print('processairport',df.columns)\n",
    "    df = process_boolean(df)\n",
    "    print('processboolean',df.columns)\n",
    "    df = process_weekday(df)\n",
    "    print('processweekday',df.columns)\n",
    "    df = process_ohe(df)\n",
    "    print('processohe',df.columns)\n",
    "    df, scaler = process_scale(df)\n",
    "    print('processscale',df.columns)\n",
    "    \n",
    "    return df, le, scaler\n",
    "\n",
    "def preprocess_features_test(df):\n",
    "    # df = process_airports_test(df)\n",
    "    df = process_boolean(df)\n",
    "    df = process_weekday(df)\n",
    "    df = process_ohe(df)\n",
    "    df = process_scale_test(df)\n",
    "    return df\n",
    "\n",
    "def prepar_data_set(data_df):\n",
    "    categoy_features = ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route']\n",
    "    numerique_features = ['DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']\n",
    "    encoders = {}\n",
    "    for col in categoy_features:\n",
    "        encoder = LabelEncoder()\n",
    "        data_df[col] = encoder.fit_transform(data_df[col])\n",
    "        encoders[col] = encoder\n",
    "    return data_df,categoy_features,numerique_features, encoders\n",
    "\n",
    "def prepare_test_set(data_df,categoy_features):\n",
    "    \n",
    "    for col in categoy_features:\n",
    "        print(col)\n",
    "        data_df[col] = encoders[col].transform(data_df[col])\n",
    "    return data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess_drop(df_train.copy())\n",
    "val = preprocess_drop(df_val.copy())\n",
    "test = preprocess_drop(df_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ATL': 0, 'BOS': 1, 'CLT': 2, 'DEN': 3, 'DFW': 4, 'DTW': 5, 'EWR': 6, 'IAD': 7, 'JFK': 8, 'LAX': 9, 'LGA': 10, 'MIA': 11, 'OAK': 12, 'ORD': 13, 'PHL': 14, 'SFO': 15}\n",
      "scale_cols ['average_price', 'average_distance', 'DepartureTimeHour', 'date_diff_days']\n",
      "df[scale_cols] Index(['startingAirport', 'destinationAirport', 'isNonStop', 'totalFare',\n",
      "       'totalTravelDistance', 'DepartureTimeHour', 'CabinCode',\n",
      "       'AirlineNameScore', 'date_diff_days', 'weekday', 'average_price',\n",
      "       'average_distance', 'route'],\n",
      "      dtype='object')\n",
      "le LabelEncoder()\n",
      "le_route LabelEncoder()\n",
      "processboolean Index(['startingAirport', 'destinationAirport', 'isNonStop', 'totalFare',\n",
      "       'totalTravelDistance', 'DepartureTimeHour', 'CabinCode',\n",
      "       'AirlineNameScore', 'date_diff_days', 'weekday', 'average_price',\n",
      "       'average_distance', 'route'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Process training data\n",
    "df_train, le_airports = process_airports(train)\n",
    "df_train, le_route = process_routes(df_train)\n",
    "df_train = process_boolean(df_train, boolean_cols)\n",
    "df_train, scaler = process_scale(df_train, scale_cols)\n",
    "# df_train, ohe = process_ohe(df_train, ohe_cols)\n",
    "\n",
    "# Process test data\n",
    "df_test = process_test_data(test, le_airports, le_route, scaler,  boolean_cols, scale_cols, ohe_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorflow nn\n",
    "def create_nn():\n",
    "    # Define the input layers\n",
    "    input_layers = []\n",
    "    output_layers = []\n",
    "    for col in categoy_features:\n",
    "        input_layer = Input(shape=(1,), name=col)\n",
    "        embedding = Embedding(input_dim=int(df_train[col].max()) + 1, output_dim=20)(input_layer)\n",
    "        embedding = Flatten()(embedding)\n",
    "        input_layers.append(input_layer)\n",
    "        output_layers.append(embedding)\n",
    "    for col in numerique_features:\n",
    "        input_layer = Input(shape=(1,), name=col)\n",
    "        input_layers.append(input_layer)\n",
    "        output_layers.append(input_layer)\n",
    "    # Concatenate the layers\n",
    "    x = Concatenate()(output_layers)\n",
    "    x = Dense(192, activation='relu')(x)\n",
    "    x = Dense(288, activation='relu')(x)\n",
    "    x = Dense(192, activation='relu')(x)\n",
    "    x = Dense(1)(x)\n",
    "    model = Model(inputs=input_layers, outputs=x)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=5,          # Number of epochs to wait for improvement\n",
    "    restore_best_weights=True  # Restore the best weights after stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 04:15:37.021513: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 04:15:37.156574: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raytechie/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route', 'DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']. Received: the structure of inputs={'startingAirport': '*', 'destinationAirport': '*', 'AirlineNameScore': '*', 'CabinCode': '*', 'DepartureTimeHour': '*', 'date_diff_days': '*', 'average_distance': '*', 'average_price': '*', 'weekday': '*', 'route': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2ms/step - loss: 18843.9355 - val_loss: 15424.2109\n",
      "Epoch 2/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 15184.3828 - val_loss: 14770.0430\n",
      "Epoch 3/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 2ms/step - loss: 14616.9707 - val_loss: 14665.5166\n",
      "Epoch 4/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 14199.7510 - val_loss: 14639.1143\n",
      "Epoch 5/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 14025.7236 - val_loss: 14428.8584\n",
      "Epoch 6/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13998.5625 - val_loss: 14151.4658\n",
      "Epoch 7/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13732.3525 - val_loss: 13825.8857\n",
      "Epoch 8/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 13599.8115 - val_loss: 14086.0635\n",
      "Epoch 9/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13595.5986 - val_loss: 13802.9424\n",
      "Epoch 10/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 13492.4004 - val_loss: 13713.1094\n",
      "Epoch 11/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13487.1045 - val_loss: 13675.5244\n",
      "Epoch 12/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13546.6768 - val_loss: 13692.6133\n",
      "Epoch 13/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 13470.2354 - val_loss: 13625.8301\n",
      "Epoch 14/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13417.0889 - val_loss: 13691.0176\n",
      "Epoch 15/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 13395.3477 - val_loss: 13650.5312\n",
      "Epoch 16/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 2ms/step - loss: 13363.8545 - val_loss: 13590.9717\n",
      "Epoch 17/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 2ms/step - loss: 13333.1689 - val_loss: 13517.8291\n",
      "Epoch 18/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 2ms/step - loss: 13319.9336 - val_loss: 13478.9824\n",
      "Epoch 19/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 2ms/step - loss: 13314.1582 - val_loss: 13508.4795\n",
      "Epoch 20/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 2ms/step - loss: 13305.0830 - val_loss: 13519.6885\n",
      "Epoch 21/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 13302.2305 - val_loss: 13501.3008\n",
      "Epoch 22/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13262.3174 - val_loss: 13486.5859\n",
      "Epoch 23/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13320.1660 - val_loss: 13544.1553\n",
      "\u001b[1m39896/39896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step\n",
      "Mean Absolute Error: 79.17790222167969\n",
      "Root Mean Squared Error: 115.14027404785156\n"
     ]
    }
   ],
   "source": [
    "# Features to use\n",
    "categoy_features = ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route']\n",
    "numerique_features = ['DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']\n",
    "# \n",
    "\n",
    "input_dict={\n",
    "    'startingAirport': df_train['startingAirport'],\n",
    "    'destinationAirport': df_train['destinationAirport'],\n",
    "    'AirlineNameScore': df_train['AirlineNameScore'],\n",
    "    'CabinCode': df_train['CabinCode'],\n",
    "    'DepartureTimeHour': df_train['DepartureTimeHour'],\n",
    "    'date_diff_days': df_train['date_diff_days'],\n",
    "    'average_distance': df_train['average_distance'],\n",
    "    'average_price': df_train['average_price'],\n",
    "    'weekday': df_train['weekday'],\n",
    "    'route': df_train['route']\n",
    "}\n",
    "\n",
    "model = create_nn()\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    input_dict, \n",
    "    df_train['totalFare'], \n",
    "    epochs=100,  # Set a high number of epochs\n",
    "    batch_size=64, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping]  # Include the EarlyStopping callback\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "# predict\n",
    "input_dict_test={\n",
    "    'startingAirport': df_test['startingAirport'],\n",
    "    'destinationAirport': df_test['destinationAirport'],\n",
    "    'AirlineNameScore': df_test['AirlineNameScore'],\n",
    "    'CabinCode': df_test['CabinCode'],\n",
    "    'DepartureTimeHour': df_test['DepartureTimeHour'],\n",
    "    'date_diff_days': df_test['date_diff_days'],\n",
    "    'average_distance': df_test['average_distance'],\n",
    "    'average_price': df_test['average_price'],\n",
    "    'weekday': df_test['weekday'],\n",
    "    'route': df_test['route']\n",
    "}\n",
    "\n",
    "y_pred = model.predict(input_dict_test)\n",
    "mae = mean_absolute_error(df_test['totalFare'], y_pred)\n",
    "rmse = root_mean_squared_error(df_test['totalFare'], y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "# Save the model\n",
    "model.save('../models/nns/nn_model.keras')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 04:15:37.021513: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 04:15:37.156574: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raytechie/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route', 'DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']. Received: the structure of inputs={'startingAirport': '*', 'destinationAirport': '*', 'AirlineNameScore': '*', 'CabinCode': '*', 'DepartureTimeHour': '*', 'date_diff_days': '*', 'average_distance': '*', 'average_price': '*', 'weekday': '*', 'route': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2ms/step - loss: 18843.9355 - val_loss: 15424.2109\n",
      "Epoch 2/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 15184.3828 - val_loss: 14770.0430\n",
      "Epoch 3/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 2ms/step - loss: 14616.9707 - val_loss: 14665.5166\n",
      "Epoch 4/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 14199.7510 - val_loss: 14639.1143\n",
      "Epoch 5/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 14025.7236 - val_loss: 14428.8584\n",
      "Epoch 6/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13998.5625 - val_loss: 14151.4658\n",
      "Epoch 7/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13732.3525 - val_loss: 13825.8857\n",
      "Epoch 8/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 13599.8115 - val_loss: 14086.0635\n",
      "Epoch 9/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13595.5986 - val_loss: 13802.9424\n",
      "Epoch 10/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 13492.4004 - val_loss: 13713.1094\n",
      "Epoch 11/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13487.1045 - val_loss: 13675.5244\n",
      "Epoch 12/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13546.6768 - val_loss: 13692.6133\n",
      "Epoch 13/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 13470.2354 - val_loss: 13625.8301\n",
      "Epoch 14/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13417.0889 - val_loss: 13691.0176\n",
      "Epoch 15/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 13395.3477 - val_loss: 13650.5312\n",
      "Epoch 16/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 2ms/step - loss: 13363.8545 - val_loss: 13590.9717\n",
      "Epoch 17/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 2ms/step - loss: 13333.1689 - val_loss: 13517.8291\n",
      "Epoch 18/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 2ms/step - loss: 13319.9336 - val_loss: 13478.9824\n",
      "Epoch 19/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 2ms/step - loss: 13314.1582 - val_loss: 13508.4795\n",
      "Epoch 20/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 2ms/step - loss: 13305.0830 - val_loss: 13519.6885\n",
      "Epoch 21/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2ms/step - loss: 13302.2305 - val_loss: 13501.3008\n",
      "Epoch 22/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13262.3174 - val_loss: 13486.5859\n",
      "Epoch 23/100\n",
      "\u001b[1m57450/57450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - loss: 13320.1660 - val_loss: 13544.1553\n",
      "\u001b[1m39896/39896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step\n",
      "Mean Absolute Error: 79.17790222167969\n",
      "Root Mean Squared Error: 115.14027404785156\n"
     ]
    }
   ],
   "source": [
    "# Features to use\n",
    "categoy_features = ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route']\n",
    "numerique_features = ['DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']\n",
    "# \n",
    "\n",
    "input_dict={\n",
    "    'startingAirport': df_train['startingAirport'],\n",
    "    'destinationAirport': df_train['destinationAirport'],\n",
    "    'AirlineNameScore': df_train['AirlineNameScore'],\n",
    "    'CabinCode': df_train['CabinCode'],\n",
    "    'DepartureTimeHour': df_train['DepartureTimeHour'],\n",
    "    'date_diff_days': df_train['date_diff_days'],\n",
    "    'average_distance': df_train['average_distance'],\n",
    "    'average_price': df_train['average_price'],\n",
    "    'weekday': df_train['weekday'],\n",
    "    'route': df_train['route']\n",
    "}\n",
    "\n",
    "model = create_nn()\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    input_dict, \n",
    "    df_train['totalFare'], \n",
    "    epochs=100,  # Set a high number of epochs\n",
    "    batch_size=64, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping]  # Include the EarlyStopping callback\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "# predict\n",
    "input_dict_test={\n",
    "    'startingAirport': df_test['startingAirport'],\n",
    "    'destinationAirport': df_test['destinationAirport'],\n",
    "    'AirlineNameScore': df_test['AirlineNameScore'],\n",
    "    'CabinCode': df_test['CabinCode'],\n",
    "    'DepartureTimeHour': df_test['DepartureTimeHour'],\n",
    "    'date_diff_days': df_test['date_diff_days'],\n",
    "    'average_distance': df_test['average_distance'],\n",
    "    'average_price': df_test['average_price'],\n",
    "    'weekday': df_test['weekday'],\n",
    "    'route': df_test['route']\n",
    "}\n",
    "\n",
    "y_pred = model.predict(input_dict_test)\n",
    "mae = mean_absolute_error(df_test['totalFare'], y_pred)\n",
    "rmse = root_mean_squared_error(df_test['totalFare'], y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "# Save the model\n",
    "model.save('../models/nns/nn_model.keras')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set predictions and metrics\n",
    "# Process the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
