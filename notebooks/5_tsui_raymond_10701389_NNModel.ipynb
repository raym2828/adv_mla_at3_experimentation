{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "import joblib\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open master df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open master\n",
    "df_train= pd.read_feather('../data/processed/train_data.feather')\n",
    "df_test= pd.read_feather('../data/processed/test_data.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average price per route\n",
    "route_avg_price = (\n",
    "    df_train.groupby(['startingAirport', 'destinationAirport'])['totalFare']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'totalFare': 'average_price'})\n",
    ")\n",
    "\n",
    "\n",
    "# Merge this back to the original dataset\n",
    "df_train = df_train.merge(route_avg_price, on=['startingAirport', 'destinationAirport'], how='left')\n",
    "df_test = df_test.merge(route_avg_price, on=['startingAirport', 'destinationAirport'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average distance to the dataset\n",
    "route_avg_distance = (\n",
    "    df_train.groupby(['startingAirport', 'destinationAirport'])['totalTravelDistance']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'totalTravelDistance': 'average_distance'})\n",
    ")\n",
    "\n",
    "df_train = df_train.merge(route_avg_distance, on=['startingAirport', 'destinationAirport'], how='left')\n",
    "df_test = df_test.merge(route_avg_distance, on=['startingAirport', 'destinationAirport'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique routes in the dataset\n",
    "# Create a new column to represent the unique route, combining airports alphabetically, store as string\n",
    "def unique_route(df):\n",
    "    df['route'] = df[['startingAirport', 'destinationAirport']].apply(\n",
    "        lambda x: str(tuple(sorted(x))), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_train = unique_route(df_train)\n",
    "df_test = unique_route(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. Encode the categories\n",
    "2. Normalise\n",
    "3. Split and train\n",
    "4. train \n",
    "5. Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess the features\n",
    "# Define the features and target\n",
    "def preprocess_drop(df):\n",
    "    df = df.drop([ 'searchDate', 'flightDate','segmentsArrivalAirportCode'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# #Label encode for airports\n",
    "# def process_airports(df):\n",
    "#     le = LabelEncoder()\n",
    "#     all_airports = sorted(set(df['startingAirport']).union(df['destinationAirport']))\n",
    "#     le.fit(all_airports)\n",
    "    \n",
    "#     df['startingAirport'] = le.transform(df['startingAirport'])\n",
    "#     df['destinationAirport'] = le.transform(df['destinationAirport'])\n",
    "#     # print dictionary of the label encoder for airports with original values and the encoded values\n",
    "#     print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "#     return df, le\n",
    "\n",
    "# def process_airports_test(df):\n",
    "\n",
    "#     df['startingAirport'] = le.transform(df['startingAirport'])\n",
    "#     df['destinationAirport'] = le.transform(df['destinationAirport'])\n",
    "#     return df\n",
    "    \n",
    "\n",
    "\n",
    "#Features to process\n",
    "boolean_cols = ['isNonStop']\n",
    "ohe_cols = ['AirlineNameScore', 'CabinCode']\n",
    "scale_cols = ['DepartureTimeHour','date_diff_days', 'CabinCode','average_distance', 'average_price']\n",
    "scale_cols = list(set(scale_cols) - set(ohe_cols))\n",
    "\n",
    "# Encode the boolean column\n",
    "def process_boolean(df):\n",
    "    df[boolean_cols] = df[boolean_cols].astype(int)\n",
    "    return df\n",
    "\n",
    "# scale data\n",
    "def process_scale(df):\n",
    "    scaler = StandardScaler()\n",
    "    df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
    "    return df, scaler\n",
    "\n",
    "def process_scale_test(df):\n",
    "    df[scale_cols] = scaler.transform(df[scale_cols])\n",
    "    return df\n",
    "\n",
    "# weekday to get cos and sine\n",
    "def process_weekday(df):\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "    df.drop('weekday', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# onehot encode cabin code\n",
    "def process_ohe(df):\n",
    "    print(\"Columns before one-hot encoding:\", df.columns)\n",
    "    print(\"Columns to one-hot encode:\", ohe_cols)\n",
    "    df = pd.get_dummies(df, columns=ohe_cols)\n",
    "    return df\n",
    "\n",
    "# Preprocess the features\n",
    "def preprocess_features(df):\n",
    "    # df, le = process_airports(df)\n",
    "    # print the columns in df \n",
    "    print('processairport',df.columns)\n",
    "    df = process_boolean(df)\n",
    "    print('processboolean',df.columns)\n",
    "    df = process_weekday(df)\n",
    "    print('processweekday',df.columns)\n",
    "    df = process_ohe(df)\n",
    "    print('processohe',df.columns)\n",
    "    df, scaler = process_scale(df)\n",
    "    print('processscale',df.columns)\n",
    "    \n",
    "    return df, le, scaler\n",
    "\n",
    "def preprocess_features_test(df):\n",
    "    # df = process_airports_test(df)\n",
    "    df = process_boolean(df)\n",
    "    df = process_weekday(df)\n",
    "    df = process_ohe(df)\n",
    "    df = process_scale_test(df)\n",
    "    return df\n",
    "\n",
    "def prepar_data_set(data_df):\n",
    "    categoy_features = ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route']\n",
    "    numerique_features = ['DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']\n",
    "    encoders = {}\n",
    "    for col in categoy_features:\n",
    "        encoder = LabelEncoder()\n",
    "        data_df[col] = encoder.fit_transform(data_df[col])\n",
    "        encoders[col] = encoder\n",
    "    return data_df,categoy_features,numerique_features, encoders\n",
    "\n",
    "def prepare_test_set(data_df):\n",
    "    categoy_features = ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route']\n",
    "    \n",
    "    for col in categoy_features:\n",
    "        print(col)\n",
    "        data_df[col] = encoders[col].transform(data_df[col])\n",
    "    return data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df_train, test_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess_drop(df_train.copy())\n",
    "val = preprocess_drop(df_val.copy())\n",
    "test = preprocess_drop(df_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startingAirport\n",
      "destinationAirport\n",
      "AirlineNameScore\n",
      "CabinCode\n",
      "route\n"
     ]
    }
   ],
   "source": [
    "#trainset\n",
    "data_df,categoy_features,numerique_features, encoders = prepar_data_set(train)\n",
    "scaler = StandardScaler()\n",
    "data_df[numerique_features] = scaler.fit_transform(data_df[numerique_features])\n",
    "\n",
    "data_df_test = prepare_test_set(test)\n",
    "data_df_test[numerique_features] = scaler.transform(data_df_test[numerique_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 22:27:22.378311: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-05 22:27:22.622204: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/raytechie/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route', 'DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']. Received: the structure of inputs={'startingAirport': '*', 'destinationAirport': '*', 'AirlineNameScore': '*', 'CabinCode': '*', 'DepartureTimeHour': '*', 'date_diff_days': '*', 'average_distance': '*', 'average_price': '*', 'weekday': '*', 'route': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81677/81706\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 19877.1641"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 40\u001b[0m\n\u001b[1;32m     26\u001b[0m input_dict\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartingAirport\u001b[39m\u001b[38;5;124m'\u001b[39m: data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartingAirport\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdestinationAirport\u001b[39m\u001b[38;5;124m'\u001b[39m: data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdestinationAirport\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroute\u001b[39m\u001b[38;5;124m'\u001b[39m: data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroute\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     37\u001b[0m }\n\u001b[1;32m     39\u001b[0m model \u001b[38;5;241m=\u001b[39m create_nn()\n\u001b[0;32m---> 40\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotalFare\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[1;32m     45\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(input_dict)\n",
      "File \u001b[0;32m~/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:344\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[1;32m    335\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m    336\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    342\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    343\u001b[0m     )\n\u001b[0;32m--> 344\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    356\u001b[0m }\n\u001b[1;32m    357\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:431\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m--> 431\u001b[0m         \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_test_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_function(iterator)\n\u001b[1;32m    433\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n",
      "File \u001b[0;32m~/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/callbacks/callback_list.py:161\u001b[0m, in \u001b[0;36mCallbackList.on_test_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    159\u001b[0m logs \u001b[38;5;241m=\u001b[39m logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 161\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_test_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/callbacks/callback.py:161\u001b[0m, in \u001b[0;36mCallback.on_test_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# For backwards compatibility.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_batch_end(batch, logs\u001b[38;5;241m=\u001b[39mlogs)\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mdefault\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_test_batch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the beginning of a batch in `evaluate` methods.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Also called at the beginning of a validation batch in the `fit`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m          method but that may change in the future.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mdefault\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_test_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create tensorflow nn\n",
    "def create_nn():\n",
    "    # Define the input layers\n",
    "    input_layers = []\n",
    "    output_layers = []\n",
    "    for col in categoy_features:\n",
    "        input_layer = Input(shape=(1,), name=col)\n",
    "        embedding = Embedding(input_dim=int(data_df[col].max()) + 1, output_dim=10)(input_layer)\n",
    "        embedding = Flatten()(embedding)\n",
    "        input_layers.append(input_layer)\n",
    "        output_layers.append(embedding)\n",
    "    for col in numerique_features:\n",
    "        input_layer = Input(shape=(1,), name=col)\n",
    "        input_layers.append(input_layer)\n",
    "        output_layers.append(input_layer)\n",
    "    # Concatenate the layers\n",
    "    x = Concatenate()(output_layers)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(1)(x)\n",
    "    model = Model(inputs=input_layers, outputs=x)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "input_dict={\n",
    "    'startingAirport': data_df['startingAirport'],\n",
    "    'destinationAirport': data_df['destinationAirport'],\n",
    "    'AirlineNameScore': data_df['AirlineNameScore'],\n",
    "    'CabinCode': data_df['CabinCode'],\n",
    "    'DepartureTimeHour': data_df['DepartureTimeHour'],\n",
    "    'date_diff_days': data_df['date_diff_days'],\n",
    "    'average_distance': data_df['average_distance'],\n",
    "    'average_price': data_df['average_price'],\n",
    "    'weekday': data_df['weekday'],\n",
    "    'route': data_df['route']\n",
    "}\n",
    "\n",
    "model = create_nn()\n",
    "model.fit(input_dict, data_df['totalFare'], epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "# predict\n",
    "input_dict_test={\n",
    "    'startingAirport': data_df_test['startingAirport'],\n",
    "    'destinationAirport': data_df_test['destinationAirport'],\n",
    "    'AirlineNameScore': data_df_test['AirlineNameScore'],\n",
    "    'CabinCode': data_df_test['CabinCode'],\n",
    "    'DepartureTimeHour': data_df_test['DepartureTimeHour'],\n",
    "    'date_diff_days': data_df_test['date_diff_days'],\n",
    "    'average_distance': data_df_test['average_distance'],\n",
    "    'average_price': data_df_test['average_price'],\n",
    "    'weekday': data_df_test['weekday'],\n",
    "    'route': data_df_test['route']\n",
    "}\n",
    "\n",
    "y_pred = model.predict(input_dict_test)\n",
    "mae = mean_absolute_error(data_df_test['totalFare'], y_pred)\n",
    "rmse = root_mean_squared_error(data_df_test['totalFare'], y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "# Save the model\n",
    "model.save('../models/nn/nn_model')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set predictions and metrics\n",
    "# Process the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
