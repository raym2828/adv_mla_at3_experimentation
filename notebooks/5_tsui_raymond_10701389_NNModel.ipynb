{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 23:19:12.676996: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-05 23:19:13.124577: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-05 23:19:14.221732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "import joblib\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open master df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open master\n",
    "df_train= pd.read_feather('../data/processed/train_data.feather')\n",
    "df_test= pd.read_feather('../data/processed/test_data.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average price per route\n",
    "route_avg_price = (\n",
    "    df_train.groupby(['startingAirport', 'destinationAirport'])['totalFare']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'totalFare': 'average_price'})\n",
    ")\n",
    "\n",
    "\n",
    "# Merge this back to the original dataset\n",
    "df_train = df_train.merge(route_avg_price, on=['startingAirport', 'destinationAirport'], how='left')\n",
    "df_test = df_test.merge(route_avg_price, on=['startingAirport', 'destinationAirport'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average distance to the dataset\n",
    "route_avg_distance = (\n",
    "    df_train.groupby(['startingAirport', 'destinationAirport'])['totalTravelDistance']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'totalTravelDistance': 'average_distance'})\n",
    ")\n",
    "\n",
    "df_train = df_train.merge(route_avg_distance, on=['startingAirport', 'destinationAirport'], how='left')\n",
    "df_test = df_test.merge(route_avg_distance, on=['startingAirport', 'destinationAirport'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique routes in the dataset\n",
    "# Create a new column to represent the unique route, combining airports alphabetically, store as string\n",
    "def unique_route(df):\n",
    "    df['route'] = df[['startingAirport', 'destinationAirport']].apply(\n",
    "        lambda x: str(tuple(sorted(x))), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_train = unique_route(df_train)\n",
    "df_test = unique_route(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. Encode the categories\n",
    "2. Normalise\n",
    "3. Split and train\n",
    "4. train \n",
    "5. Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess the features\n",
    "# Define the features and target\n",
    "def preprocess_drop(df):\n",
    "    df = df.drop([ 'searchDate', 'flightDate','segmentsArrivalAirportCode'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# #Label encode for airports\n",
    "# def process_airports(df):\n",
    "#     le = LabelEncoder()\n",
    "#     all_airports = sorted(set(df['startingAirport']).union(df['destinationAirport']))\n",
    "#     le.fit(all_airports)\n",
    "    \n",
    "#     df['startingAirport'] = le.transform(df['startingAirport'])\n",
    "#     df['destinationAirport'] = le.transform(df['destinationAirport'])\n",
    "#     # print dictionary of the label encoder for airports with original values and the encoded values\n",
    "#     print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "#     return df, le\n",
    "\n",
    "# def process_airports_test(df):\n",
    "\n",
    "#     df['startingAirport'] = le.transform(df['startingAirport'])\n",
    "#     df['destinationAirport'] = le.transform(df['destinationAirport'])\n",
    "#     return df\n",
    "    \n",
    "\n",
    "\n",
    "#Features to process\n",
    "boolean_cols = ['isNonStop']\n",
    "ohe_cols = ['AirlineNameScore', 'CabinCode']\n",
    "scale_cols = ['DepartureTimeHour','date_diff_days', 'CabinCode','average_distance', 'average_price']\n",
    "scale_cols = list(set(scale_cols) - set(ohe_cols))\n",
    "\n",
    "# Encode the boolean column\n",
    "def process_boolean(df):\n",
    "    df[boolean_cols] = df[boolean_cols].astype(int)\n",
    "    return df\n",
    "\n",
    "# scale data\n",
    "def process_scale(df):\n",
    "    scaler = StandardScaler()\n",
    "    df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
    "    return df, scaler\n",
    "\n",
    "def process_scale_test(df):\n",
    "    df[scale_cols] = scaler.transform(df[scale_cols])\n",
    "    return df\n",
    "\n",
    "# weekday to get cos and sine\n",
    "def process_weekday(df):\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "    df.drop('weekday', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# onehot encode cabin code\n",
    "def process_ohe(df):\n",
    "    print(\"Columns before one-hot encoding:\", df.columns)\n",
    "    print(\"Columns to one-hot encode:\", ohe_cols)\n",
    "    df = pd.get_dummies(df, columns=ohe_cols)\n",
    "    return df\n",
    "\n",
    "# Preprocess the features\n",
    "def preprocess_features(df):\n",
    "    # df, le = process_airports(df)\n",
    "    # print the columns in df \n",
    "    print('processairport',df.columns)\n",
    "    df = process_boolean(df)\n",
    "    print('processboolean',df.columns)\n",
    "    df = process_weekday(df)\n",
    "    print('processweekday',df.columns)\n",
    "    df = process_ohe(df)\n",
    "    print('processohe',df.columns)\n",
    "    df, scaler = process_scale(df)\n",
    "    print('processscale',df.columns)\n",
    "    \n",
    "    return df, le, scaler\n",
    "\n",
    "def preprocess_features_test(df):\n",
    "    # df = process_airports_test(df)\n",
    "    df = process_boolean(df)\n",
    "    df = process_weekday(df)\n",
    "    df = process_ohe(df)\n",
    "    df = process_scale_test(df)\n",
    "    return df\n",
    "\n",
    "def prepar_data_set(data_df):\n",
    "    categoy_features = ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route']\n",
    "    numerique_features = ['DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']\n",
    "    encoders = {}\n",
    "    for col in categoy_features:\n",
    "        encoder = LabelEncoder()\n",
    "        data_df[col] = encoder.fit_transform(data_df[col])\n",
    "        encoders[col] = encoder\n",
    "    return data_df,categoy_features,numerique_features, encoders\n",
    "\n",
    "def prepare_test_set(data_df):\n",
    "    categoy_features = ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route']\n",
    "    \n",
    "    for col in categoy_features:\n",
    "        print(col)\n",
    "        data_df[col] = encoders[col].transform(data_df[col])\n",
    "    return data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df_train, test_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess_drop(df_train.copy())\n",
    "val = preprocess_drop(df_val.copy())\n",
    "test = preprocess_drop(df_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startingAirport\n",
      "destinationAirport\n",
      "AirlineNameScore\n",
      "CabinCode\n",
      "route\n"
     ]
    }
   ],
   "source": [
    "#trainset\n",
    "data_df,categoy_features,numerique_features, encoders = prepar_data_set(train)\n",
    "scaler = StandardScaler()\n",
    "data_df[numerique_features] = scaler.fit_transform(data_df[numerique_features])\n",
    "\n",
    "data_df_test = prepare_test_set(test)\n",
    "data_df_test[numerique_features] = scaler.transform(data_df_test[numerique_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 23:19:45.648336: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-05 23:19:45.896231: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raytechie/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['startingAirport', 'destinationAirport', 'AirlineNameScore', 'CabinCode', 'route', 'DepartureTimeHour', 'date_diff_days', 'average_distance', 'average_price', 'weekday']. Received: the structure of inputs={'startingAirport': '*', 'destinationAirport': '*', 'AirlineNameScore': '*', 'CabinCode': '*', 'DepartureTimeHour': '*', 'date_diff_days': '*', 'average_distance': '*', 'average_price': '*', 'weekday': '*', 'route': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12767/12767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - loss: 32554.7109 - val_loss: 20135.9668\n",
      "Epoch 2/10\n",
      "\u001b[1m12767/12767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 24085.2871 - val_loss: 19876.9297\n",
      "Epoch 3/10\n",
      "\u001b[1m12767/12767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 23465.6895 - val_loss: 19979.5762\n",
      "Epoch 4/10\n",
      "\u001b[1m12767/12767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 23422.5977 - val_loss: 19255.8301\n",
      "Epoch 5/10\n",
      "\u001b[1m12767/12767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 23077.3574 - val_loss: 19816.5469\n",
      "Epoch 6/10\n",
      "\u001b[1m12767/12767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - loss: 22766.9531 - val_loss: 18943.2285\n",
      "Epoch 7/10\n",
      "\u001b[1m12767/12767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 22510.0566 - val_loss: 18827.3926\n",
      "Epoch 8/10\n",
      "\u001b[1m12767/12767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 21837.4180 - val_loss: 18748.8281\n",
      "Epoch 9/10\n",
      "\u001b[1m12767/12767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - loss: 21717.6406 - val_loss: 19095.1543\n",
      "Epoch 10/10\n",
      "\u001b[1m12767/12767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - loss: 21681.2559 - val_loss: 18535.4453\n",
      "\u001b[1m39896/39896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1ms/step\n",
      "Mean Absolute Error: 95.9054183959961\n",
      "Root Mean Squared Error: 134.43017578125\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=../models/nn/nn_model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoot Mean Squared Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../models/nn/nn_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Projects/adv_mla_at3/ml_experimentation/.venv/lib/python3.10/site-packages/keras/src/saving/saving_api.py:114\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    112\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[1;32m    113\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filepath extension for saving. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=../models/nn/nn_model."
     ]
    }
   ],
   "source": [
    "# Create tensorflow nn\n",
    "def create_nn():\n",
    "    # Define the input layers\n",
    "    input_layers = []\n",
    "    output_layers = []\n",
    "    for col in categoy_features:\n",
    "        input_layer = Input(shape=(1,), name=col)\n",
    "        embedding = Embedding(input_dim=int(data_df[col].max()) + 1, output_dim=10)(input_layer)\n",
    "        embedding = Flatten()(embedding)\n",
    "        input_layers.append(input_layer)\n",
    "        output_layers.append(embedding)\n",
    "    for col in numerique_features:\n",
    "        input_layer = Input(shape=(1,), name=col)\n",
    "        input_layers.append(input_layer)\n",
    "        output_layers.append(input_layer)\n",
    "    # Concatenate the layers\n",
    "    x = Concatenate()(output_layers)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)   \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1)(x)\n",
    "    model = Model(inputs=input_layers, outputs=x)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "input_dict={\n",
    "    'startingAirport': data_df['startingAirport'],\n",
    "    'destinationAirport': data_df['destinationAirport'],\n",
    "    'AirlineNameScore': data_df['AirlineNameScore'],\n",
    "    'CabinCode': data_df['CabinCode'],\n",
    "    'DepartureTimeHour': data_df['DepartureTimeHour'],\n",
    "    'date_diff_days': data_df['date_diff_days'],\n",
    "    'average_distance': data_df['average_distance'],\n",
    "    'average_price': data_df['average_price'],\n",
    "    'weekday': data_df['weekday'],\n",
    "    'route': data_df['route']\n",
    "}\n",
    "\n",
    "model = create_nn()\n",
    "history = model.fit(input_dict, data_df['totalFare'], epochs=10, batch_size=32, validation_split=0.2)\n",
    "# Evaluate the model\n",
    "# predict\n",
    "input_dict_test={\n",
    "    'startingAirport': data_df_test['startingAirport'],\n",
    "    'destinationAirport': data_df_test['destinationAirport'],\n",
    "    'AirlineNameScore': data_df_test['AirlineNameScore'],\n",
    "    'CabinCode': data_df_test['CabinCode'],\n",
    "    'DepartureTimeHour': data_df_test['DepartureTimeHour'],\n",
    "    'date_diff_days': data_df_test['date_diff_days'],\n",
    "    'average_distance': data_df_test['average_distance'],\n",
    "    'average_price': data_df_test['average_price'],\n",
    "    'weekday': data_df_test['weekday'],\n",
    "    'route': data_df_test['route']\n",
    "}\n",
    "\n",
    "y_pred = model.predict(input_dict_test)\n",
    "mae = mean_absolute_error(data_df_test['totalFare'], y_pred)\n",
    "rmse = root_mean_squared_error(data_df_test['totalFare'], y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "# Save the model\n",
    "model.save('../models/nn/nn_model')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set predictions and metrics\n",
    "# Process the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
